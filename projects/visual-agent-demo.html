<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Visual AI Agent Demo</title>

<!-- MediaPipe -->
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_detection/face_detection.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>

<!-- Particles.js -->
<script src="https://cdn.jsdelivr.net/npm/particles.js"></script>

<style>
  body {
    font-family: sans-serif;
    margin: 0;
    background: #121212;
    color: #fff;
    text-align: center;
    overflow-x: hidden;
  }
  #particles-js {
    position: fixed;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    z-index: -1;
  }
  .container {
    display: flex;
    justify-content: center;
    gap: 20px;
    margin-top: 20px;
    flex-wrap: wrap;
    align-items: flex-start;
  }
  #cameraBox {
    position: relative;
    display: inline-block;
  }
  #webcam, #overlay {
    position: absolute;
    top: 0;
    left: 0;
  }
  #webcam { opacity: 0; } /* hide live video, only overlay shows paused frame */
  canvas { border: 2px solid #fff; }
  iframe { width: 600px; height: 480px; border: 2px solid #ccc; background: #fff; }
  button { padding: 10px 20px; font-size: 16px; margin-top: 20px; cursor: pointer; color: #000; }
</style>
</head>
<body>

<div id="particles-js"></div>

<h1>Visual AI Agent Demo</h1>
<div class="container">
  <div id="cameraBox">
    <video id="webcam" autoplay playsinline width="640" height="480"></video>
    <canvas id="overlay" width="640" height="480"></canvas>
    <br>
    <button id="restartButton">Restart Camera</button>
  </div>
  <iframe id="wikiFrame" src="about:blank" sandbox="allow-scripts allow-same-origin"></iframe>
</div>

<script>
// Particles.js config
particlesJS("particles-js", {
  particles: {
    number: { value: 80 },
    color: { value: "#ffffff" },
    shape: { type: "circle" },
    opacity: { value: 0.5 },
    size: { value: 3 },
    line_linked: { enable: true, distance: 150, color: "#ffffff", opacity: 0.4, width: 1 },
    move: { enable: true, speed: 2 }
  },
  interactivity: { events: { onhover: { enable: true, mode: "repulse" } } }
});

const video = document.getElementById('webcam');
const canvas = document.getElementById('overlay');
const ctx = canvas.getContext('2d');
const iframe = document.getElementById('wikiFrame');
const restartBtn = document.getElementById('restartButton');

let camera = null;
let detectedOnce = false;
let pausedFrame = null;

// Data science terms
const dsTerms = [
  "Data science","Machine learning","Artificial intelligence","Deep learning",
  "Neural networks","Big data","Statistics","Predictive analytics",
  "Natural language processing","Computer vision","Reinforcement learning",
  "Supervised learning","Unsupervised learning","Clustering","Regression analysis"
];

// Pick a random DS term
function getRandomTerm() {
    return dsTerms[Math.floor(Math.random() * dsTerms.length)];
}

// MediaPipe Face Detection
const faceDetection = new FaceDetection({
    locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_detection/${file}`
});
faceDetection.setOptions({ model: 'short', minDetectionConfidence: 0.5 });

faceDetection.onResults(results => {
    // If already paused, draw frozen frame
    if (pausedFrame) {
        ctx.putImageData(pausedFrame, 0, 0);
        return;
    }

    // Draw live feed frame
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

    // Draw bounding boxes
    results.detections.forEach(detection => {
        const box = detection.boundingBox;
        ctx.strokeStyle = 'lime';
        ctx.lineWidth = 3;
        ctx.strokeRect(
          box.xCenter - box.width/2,
          box.yCenter - box.height/2,
          box.width,
          box.height
        );
    });

    if (results.detections.length > 0 && !detectedOnce) {
        detectedOnce = true;

        // Capture paused frame
        pausedFrame = ctx.getImageData(0, 0, canvas.width, canvas.height);

        stopCamera();
        triggerAction();
    }
});

// Start camera
function startCamera() {
    detectedOnce = false;
    pausedFrame = null;
    camera = new Camera(video, {
        onFrame: async () => { await faceDetection.send({image: video}); },
        width: 640,
        height: 480
    });
    camera.start();
}

// Stop camera (pause at current frame)
function stopCamera() {
    if (camera) {
        camera.stop();
        camera = null;
        console.log("Camera paused on first detection.");
    }
}

// Trigger search → Wikipedia → force light mode
function triggerAction() {
    const term = getRandomTerm();
    iframe.src = `https://www.google.com/search?q=${encodeURIComponent(term)}+site:en.wikipedia.org&btnI=I`;
    console.log(`Face detected! Searching for Wikipedia article: ${term}`);
}

// Ensure Wikipedia loads in light mode
iframe.onload = () => {
  if (iframe.src.includes("wikipedia.org") && !iframe.src.includes("safemode=1")) {
    iframe.src = iframe.src + (iframe.src.includes("?") ? "&" : "?") + "safemode=1";
  }
};

// Restart button
restartBtn.addEventListener('click', () => {
    startCamera();
    console.log("Camera restarted.");
});

// Start camera on load
startCamera();
</script>
</body>
</html>
